"""
IRIS Voice Input - –°—É–ø–µ—Ä-–ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è –≤–µ—Ä—Å–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
–í–µ—Ä—Å–∏—è: 5.0.0 - –ò–ò-–ú–æ–∑–≥ —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏
"""

import asyncio
import threading
import time
import queue
import os
import json
import logging
import sys
import pickle
import hashlib
import uuid
from typing import Optional, Callable, List, Dict, Any, Tuple, Union, Set
from pathlib import Path
from dataclasses import dataclass, asdict, field
from enum import Enum, auto
from datetime import datetime, timedelta
from collections import defaultdict, deque
import numpy as np
import wave
import io

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º –∞–Ω–∞–ª–∏–∑–æ–º
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('iris_ai.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger('IRIS-AI')

# ============================================
# –ö–û–ù–°–¢–ê–ù–¢–´ –ò –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ò–ò-–ú–û–î–£–õ–Ø
# ============================================

AI_MODES = {
    "ADAPTIVE": "adaptive",           # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º
    "NEURAL": "neural",               # –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–π —Ä–µ–∂–∏–º
    "CONTEXTUAL": "contextual",       # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ-–∑–∞–≤–∏—Å–∏–º—ã–π
    "EMOTIONAL": "emotional",         # –° —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç–æ–º
    "MULTIMODAL": "multimodal",       # –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π (–≥–æ–ª–æ—Å + —Ç–µ–∫—Å—Ç)
    "AUTONOMOUS": "autonomous"        # –ê–≤—Ç–æ–Ω–æ–º–Ω–æ–µ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π
}

EMOTION_TYPES = {
    "NEUTRAL": "neutral",
    "HAPPY": "happy",
    "SAD": "sad",
    "ANGRY": "angry",
    "EXCITED": "excited",
    "CALM": "calm",
    "STRESSED": "stressed",
    "CONFUSED": "confused"
}

CONTEXT_DOMAINS = {
    "WEATHER": "weather",
    "MUSIC": "music",
    "NEWS": "news",
    "SMART_HOME": "smart_home",
    "SCHEDULE": "schedule",
    "COMMUNICATION": "communication",
    "ENTERTAINMENT": "entertainment",
    "PRODUCTIVITY": "productivity",
    "LEARNING": "learning",
    "HEALTH": "health"
}

# ============================================
# –î–ò–ù–ê–ú–ò–ß–ï–°–ö–ò–ï –ò–ú–ü–û–†–¢–´ –ú–ê–®–ò–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø
# ============================================

# –ë–∞–∑–æ–≤—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
try:
    import numpy as np
    NP_AVAILABLE = True
except ImportError:
    NP_AVAILABLE = False
    logger.warning("NumPy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install numpy")

# –ê—É–¥–∏–æ–æ–±—Ä–∞–±–æ—Ç–∫–∞
try:
    import sounddevice as sd
    import soundfile as sf
    SOUND_AVAILABLE = True
except ImportError:
    SOUND_AVAILABLE = False
    logger.warning("SoundDevice/SoundFile –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

try:
    import pyaudio
    import wave
    PYAUDIO_AVAILABLE = True
except ImportError:
    PYAUDIO_AVAILABLE = False

# –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–µ—á–∏
try:
    from vosk import Model, KaldiRecognizer, SetLogLevel
    SetLogLevel(-1)
    VOSK_AVAILABLE = True
except ImportError:
    VOSK_AVAILABLE = False

try:
    import speech_recognition as sr
    SR_AVAILABLE = True
except ImportError:
    SR_AVAILABLE = False

# –Ø–Ω–¥–µ–∫—Å SpeechKit
try:
    import requests
    import uuid as uuid_lib
    YANDEX_AVAILABLE = True
except ImportError:
    YANDEX_AVAILABLE = False

# –ú–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
ML_LIBS = {}
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.utils.data import Dataset, DataLoader
    ML_LIBS['PYTORCH'] = True
except ImportError:
    ML_LIBS['PYTORCH'] = False

try:
    import tensorflow as tf
    ML_LIBS['TENSORFLOW'] = True
except ImportError:
    ML_LIBS['TENSORFLOW'] = False

try:
    import sklearn
    from sklearn.cluster import KMeans
    from sklearn.decomposition import PCA
    from sklearn.preprocessing import StandardScaler
    ML_LIBS['SKLEARN'] = True
except ImportError:
    ML_LIBS['SKLEARN'] = False

# NLP –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞
try:
    import nltk
    from nltk.tokenize import word_tokenize
    from nltk.corpus import stopwords
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
    NLP_AVAILABLE = True
except ImportError:
    NLP_AVAILABLE = False

# –ê—É–¥–∏–æ–æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–∞—è
try:
    import librosa
    import librosa.display
    AUDIO_ML_AVAILABLE = True
except ImportError:
    AUDIO_ML_AVAILABLE = False

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
try:
    import matplotlib.pyplot as plt
    from matplotlib.animation import FuncAnimation
    VISUAL_AVAILABLE = True
except ImportError:
    VISUAL_AVAILABLE = False

# ============================================
# –î–ê–¢–ê–ö–õ–ê–°–°–´ –î–õ–Ø –ò–ò-–°–¢–†–£–ö–¢–£–†
# ============================================

@dataclass
class NeuralConfig:
    """–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤–æ–≥–æ –º–æ–¥—É–ª—è"""
    use_attention: bool = True
    use_transformer: bool = True
    hidden_layers: int = 4
    neurons_per_layer: int = 256
    dropout_rate: float = 0.3
    learning_rate: float = 0.001
    batch_size: int = 32
    epochs: int = 100
    use_pretrained: bool = True
    model_type: str = "transformer"  # transformer, lstm, cnn, hybrid
    feature_size: int = 128
    context_window: int = 10

@dataclass
class EmotionState:
    """–°–æ—Å—Ç–æ—è–Ω–∏–µ —ç–º–æ—Ü–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    emotion: str = "neutral"
    confidence: float = 0.0
    intensity: float = 0.0
    valence: float = 0.0  # –ü–æ–∑–∏—Ç–∏–≤–Ω–æ—Å—Ç—å
    arousal: float = 0.0  # –í–æ–∑–±—É–∂–¥–µ–Ω–∏–µ
    dominance: float = 0.0  # –î–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ
    timestamp: float = field(default_factory=time.time)
    history: List[Dict] = field(default_factory=list)

@dataclass
class UserProfile:
    """–ü—Ä–æ—Ñ–∏–ª—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π"""
    user_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    voice_features: Dict = field(default_factory=dict)
    speech_patterns: Dict = field(default_factory=dict)
    preferences: Dict = field(default_factory=dict)
    learning_rate: float = 0.1
    adaptation_level: float = 0.0
    interaction_count: int = 0
    last_interaction: float = field(default_factory=time.time)
    created_at: float = field(default_factory=time.time)

@dataclass
class AIContext:
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    current_domain: str = ""
    previous_commands: List[str] = field(default_factory=list)
    user_intent: str = ""
    entities: List[Dict] = field(default_factory=list)
    conversation_history: List[Dict] = field(default_factory=list)
    memory: Dict = field(default_factory=dict)
    context_score: float = 0.0
    temporal_context: Dict = field(default_factory=dict)

@dataclass
class LearningData:
    """–î–∞–Ω–Ω—ã–µ –¥–ª—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è"""
    audio_samples: List = field(default_factory=list)
    transcriptions: List = field(default_factory=list)
    corrections: List = field(default_factory=list)
    success_patterns: List = field(default_factory=list)
    error_patterns: List = field(default_factory=list)
    reinforcement_signals: List = field(default_factory=list)

@dataclass
class PerformanceMetrics:
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    real_time_factor: float = 0.0
    latency: Dict = field(default_factory=lambda: {"audio": 0.0, "processing": 0.0})
    accuracy: Dict = field(default_factory=lambda: {"wake": 0.0, "command": 0.0})
    efficiency: Dict = field(default_factory=lambda: {"cpu": 0.0, "memory": 0.0})
    quality: Dict = field(default_factory=lambda: {"audio": 0.0, "recognition": 0.0})

# ============================================
# –ù–ï–ô–†–û–°–ï–¢–ï–í–´–ï –ú–û–î–£–õ–ò
# ============================================

class VoiceEncoder(nn.Module):
    """–ù–µ–π—Ä–æ—Å–µ—Ç—å –¥–ª—è –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤—ã—Ö –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–µ–π"""
    def __init__(self, input_dim=128, hidden_dim=256, latent_dim=64):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, latent_dim)
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        latent = self.encoder(x)
        reconstructed = self.decoder(latent)
        return latent, reconstructed

class EmotionClassifier(nn.Module):
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —ç–º–æ—Ü–∏–π –ø–æ –≥–æ–ª–æ—Å—É"""
    def __init__(self, input_dim=128, num_emotions=8):
        super().__init__()
        self.conv1 = nn.Conv1d(1, 32, kernel_size=5, stride=2)
        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=2)
        self.conv3 = nn.Conv1d(64, 128, kernel_size=5, stride=2)
        
        self.lstm = nn.LSTM(128, 128, batch_first=True, bidirectional=True)
        
        self.fc = nn.Sequential(
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_emotions),
            nn.Softmax(dim=1)
        )
        
    def forward(self, x):
        x = x.unsqueeze(1)  # Add channel dimension
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        
        x = x.transpose(1, 2)
        lstm_out, _ = self.lstm(x)
        
        # Use last hidden state
        last_hidden = lstm_out[:, -1, :]
        emotion_probs = self.fc(last_hidden)
        return emotion_probs

class IntentRecognizer:
    """–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º NLP"""
    def __init__(self):
        self.intent_patterns = {
            "–≤–∫–ª—é—á–∏—Ç—å": ["–≤–∫–ª—é—á–∏", "–∑–∞–ø—É—Å—Ç–∏", "–∞–∫—Ç–∏–≤–∏—Ä—É–π", "–≤—Ä—É–±–∏", "—Å—Ç–∞—Ä—Ç"],
            "–≤—ã–∫–ª—é—á–∏—Ç—å": ["–≤—ã–∫–ª—é—á–∏", "–æ—Å—Ç–∞–Ω–æ–≤–∏", "–¥–µ–∞–∫—Ç–∏–≤–∏—Ä—É–π", "–≤—ã—Ä—É–±–∏", "—Å—Ç–æ–ø"],
            "—É–∑–Ω–∞—Ç—å": ["—Å–∫–æ–ª—å–∫–æ", "–∫–∞–∫–∞—è", "–∫–∞–∫–æ–π", "—á—Ç–æ", "–∫—Ç–æ", "–≥–¥–µ", "–∫–æ–≥–¥–∞"],
            "–∏–∑–º–µ–Ω–∏—Ç—å": ["–∏–∑–º–µ–Ω–∏", "–Ω–∞—Å—Ç—Ä–æ–π", "—Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–π", "–ø–æ–ø—Ä–∞–≤—å"],
            "–Ω–∞–π—Ç–∏": ["–Ω–∞–π–¥–∏", "–ø–æ–∏—â–∏", "–∏—â–∏", "–æ—Ç—ã—â–∏", "–ª–æ–∫–∞–ª–∏–∑—É–π"],
            "—Å–æ–∑–¥–∞—Ç—å": ["—Å–æ–∑–¥–∞–π", "—Å–¥–µ–ª–∞–π", "–ø–æ—Å—Ç—Ä–æ–π", "–æ—Ä–≥–∞–Ω–∏–∑—É–π", "–ø—Ä–∏–¥—É–º–∞–π"],
            "—É–¥–∞–ª–∏—Ç—å": ["—É–¥–∞–ª–∏", "—É–±–µ—Ä–∏", "—Å—Ç–µ—Ä–Ω–∏", "–ª–∏–∫–≤–∏–¥–∏—Ä—É–π", "—Å–æ—Ç—Ä–∏"],
            "–ø–æ–º–æ—á—å": ["–ø–æ–º–æ–≥–∏", "–ø–æ–¥—Å–∫–∞–∂–∏", "–æ–±—ä—è—Å–Ω–∏", "–ø–æ—Å–æ–≤–µ—Ç—É–π", "—Ä–∞—Å—Å–∫–∞–∂–∏"]
        }
        
        self.entity_types = {
            "—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ": ["—Ç–µ–ª–µ–≤–∏–∑–æ—Ä", "—Å–≤–µ—Ç", "–ª–∞–º–ø–∞", "–∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä", "–æ–±–æ–≥—Ä–µ–≤–∞—Ç–µ–ª—å"],
            "–º–µ–¥–∏–∞": ["–º—É–∑—ã–∫–∞", "—Ñ–∏–ª—å–º", "–≤–∏–¥–µ–æ", "—Ä–∞–¥–∏–æ", "–ø–æ–¥–∫–∞—Å—Ç"],
            "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è": ["–ø–æ–≥–æ–¥–∞", "–Ω–æ–≤–æ—Å—Ç–∏", "–∫—É—Ä—Å", "–≤—Ä–µ–º—è", "–¥–∞—Ç–∞"],
            "–Ω–∞—Å—Ç—Ä–æ–π–∫–∞": ["–≥—Ä–æ–º–∫–æ—Å—Ç—å", "—è—Ä–∫–æ—Å—Ç—å", "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞", "—Å–∫–æ—Ä–æ—Å—Ç—å"]
        }
        
    def extract_intent(self, text: str) -> Dict:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–º–µ—Ä–µ–Ω–∏—è –∏ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ —Ç–µ–∫—Å—Ç–∞"""
        result = {
            "intent": "unknown",
            "confidence": 0.0,
            "entities": [],
            "action": "",
            "target": ""
        }
        
        text_lower = text.lower()
        words = text_lower.split()
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω—Ç–µ–Ω—Ç
        for intent, patterns in self.intent_patterns.items():
            for pattern in patterns:
                if pattern in text_lower:
                    result["intent"] = intent
                    result["confidence"] = 0.8
                    result["action"] = pattern
                    break
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏
        for entity_type, entities in self.entity_types.items():
            for entity in entities:
                if entity in text_lower:
                    result["entities"].append({
                        "type": entity_type,
                        "value": entity,
                        "position": text_lower.find(entity)
                    })
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ª—å (–ø–æ—Å–ª–µ–¥–Ω–µ–µ —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ)
        if NLP_AVAILABLE:
            try:
                tokens = word_tokenize(text_lower, language='russian')
                pos_tags = nltk.pos_tag(tokens, lang='rus')
                
                for word, tag in pos_tags:
                    if tag.startswith('S'):  # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ–µ
                        result["target"] = word
            except:
                pass
        
        return result

# ============================================
# –û–°–ù–û–í–ù–û–ô –ö–õ–ê–°–° IRIS AI
# ============================================

class IRISVoiceAI:
    """
    –°—É–ø–µ—Ä-–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º –∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏
    """
    
    # Wake word –≤–∞—Ä–∏–∞—Ü–∏–∏ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ–º
    WAKE_WORD_VARIANTS = [
        '–∏—Ä–∏—Å', 'iris', '–∏—Ä–∏', '–∏—Ä–∏—Å–∫–∞', '–∏—Ä–∏—Å—Å', '–∏—Ä–∏—Å–∞',
        '–∞–π—Ä–∏—Å', '–∞—Ä–∏—Å', '–∏—Ä–∏—à', '–∏—Ä–∏—Å—å', '—Ä–∏—Å', '—ç—Ä–∏—Å',
        '–∏—Ä–∏—Å—é', '–∏—Ä–∏—Å—è', '–∏—Ä–∏—Å—É', '–∏—Ä–∏—Å–µ', '–∏—Ä–∏—à–∞'
    ]
    
    # –ö–æ–º–∞–Ω–¥—ã —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –ø–æ–Ω–∏–º–∞–Ω–∏–µ–º
    SMART_COMMANDS = {
        '–ø–æ–≥–æ–¥–∞': {
            'actions': ['–ø–æ–≥–æ–¥–∞', '–ø—Ä–æ–≥–Ω–æ–∑', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞', '–¥–æ–∂–¥—å', '—Å–æ–ª–Ω—Ü–µ'],
            'context': 'weather',
            'requires_location': True
        },
        '–º—É–∑—ã–∫–∞': {
            'actions': ['–º—É–∑—ã–∫–∞', '–ø–µ—Å–Ω—è', '—Ç—Ä–µ–∫', '–∞–ª—å–±–æ–º', '–ø–ª–µ–π–ª–∏—Å—Ç'],
            'context': 'music',
            'requires_query': True
        },
        '–Ω–æ–≤–æ—Å—Ç–∏': {
            'actions': ['–Ω–æ–≤–æ—Å—Ç–∏', '—Å–æ–±—ã—Ç–∏—è', '–ø—Ä–æ–∏—Å—à–µ—Å—Ç–≤–∏—è', '—Å–≤–æ–¥–∫–∞'],
            'context': 'news',
            'category': 'general'
        },
        '—É–º–Ω—ã–π –¥–æ–º': {
            'actions': ['–≤–∫–ª—é—á–∏', '–≤—ã–∫–ª—é—á–∏', '—Å–≤–µ—Ç', '—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞', '—Ä–æ–∑–µ—Ç–∫–∞'],
            'context': 'smart_home',
            'devices': ['—Å–≤–µ—Ç', '–ª–∞–º–ø–∞', '—Ç–µ–ª–µ–≤–∏–∑–æ—Ä', '–∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä']
        }
    }
    
    def __init__(self, 
                 config_path: Optional[str] = None,
                 ai_mode: str = "adaptive",
                 neural_config: Optional[NeuralConfig] = None,
                 enable_self_learning: bool = True,
                 enable_emotion_recognition: bool = True,
                 enable_context_awareness: bool = True):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        
        Args:
            config_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            ai_mode: –†–µ–∂–∏–º –ò–ò (adaptive, neural, contextual, emotional, multimodal, autonomous)
            neural_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π
            enable_self_learning: –í–∫–ª—é—á–∏—Ç—å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ
            enable_emotion_recognition: –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π
            enable_context_awareness: –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç—å
        """
        
        print("‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó")
        print("‚ïë         üß† –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø IRIS AI –ú–û–ó–ì–ê                ‚ïë")
        print("‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù")
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
        self.ai_mode = ai_mode
        self.enable_self_learning = enable_self_learning
        self.enable_emotion_recognition = enable_emotion_recognition
        self.enable_context_awareness = enable_context_awareness
        
        # –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
        self.neural_config = neural_config or NeuralConfig()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
        self._init_paths()
        self._init_components()
        self._init_neural_networks()
        self._init_ai_modules()
        
        # –°–æ—Å—Ç–æ—è–Ω–∏—è –ò–ò
        self.emotion_state = EmotionState()
        self.user_profile = UserProfile()
        self.ai_context = AIContext()
        self.learning_data = LearningData()
        self.performance_metrics = PerformanceMetrics()
        
        # –°–∏—Å—Ç–µ–º–∞ –æ—á–µ—Ä–µ–¥–µ–π –∏ –ø–æ—Ç–æ–∫–æ–≤
        self.command_queue = queue.PriorityQueue()
        self.audio_queue = queue.Queue()
        self.event_queue = asyncio.Queue()
        
        # –ü–æ—Ç–æ–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.threads = {}
        self.is_running = False
        self.is_learning = False
        
        # –ö–æ–ª–ª–±—ç–∫–∏ –∏ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        self.callbacks = {
            'command': [],
            'wake': [],
            'error': [],
            'emotion_change': [],
            'context_change': [],
            'learning_update': [],
            'intent_detected': []
        }
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if config_path and os.path.exists(config_path):
            self._load_config(config_path)
        
        # –í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ
        self._print_system_info()
        
        print("‚úÖ IRIS AI –ú–æ–∑–≥ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
        print("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
    
    def _init_paths(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—É—Ç–µ–π –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –ò–ò"""
        self.base_dir = Path.home() / ".iris_ai"
        self.base_dir.mkdir(exist_ok=True)
        
        self.paths = {
            'models': self.base_dir / "models",
            'profiles': self.base_dir / "profiles",
            'learning': self.base_dir / "learning",
            'audio': self.base_dir / "audio_samples",
            'logs': self.base_dir / "logs",
            'config': self.base_dir / "config"
        }
        
        for path in self.paths.values():
            path.mkdir(exist_ok=True)
    
    def _init_components(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã"""
        print("[IRIS AI] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏
        self.speech_engines = {}
        
        if VOSK_AVAILABLE:
            self._init_vosk()
        
        if SR_AVAILABLE:
            self._init_speech_recognition()
        
        if YANDEX_AVAILABLE:
            self._init_yandex_speechkit()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ
        self._init_audio_system()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NLP
        if NLP_AVAILABLE:
            self.intent_recognizer = IntentRecognizer()
            print("[IRIS AI] ‚úÖ NLP –º–æ–¥—É–ª—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        print("[IRIS AI] ‚úÖ –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    
    def _init_neural_networks(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        print("[IRIS AI] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥—É–ª–µ–π...")
        
        self.neural_models = {}
        
        if ML_LIBS.get('PYTORCH', False):
            try:
                # –ì–æ–ª–æ—Å–æ–≤–æ–π —ç–Ω–∫–æ–¥–µ—Ä
                self.neural_models['voice_encoder'] = VoiceEncoder()
                
                # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä —ç–º–æ—Ü–∏–π
                self.neural_models['emotion_classifier'] = EmotionClassifier()
                
                # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤, –µ—Å–ª–∏ –µ—Å—Ç—å
                self._load_neural_weights()
                
                print("[IRIS AI] ‚úÖ –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã (PyTorch)")
            except Exception as e:
                print(f"[IRIS AI] ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: {e}")
        
        if ML_LIBS.get('SKLEARN', False):
            try:
                # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è –≥–æ–ª–æ—Å–æ–≤—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
                self.neural_models['voice_cluster'] = KMeans(n_clusters=5)
                self.neural_models['feature_scaler'] = StandardScaler()
                print("[IRIS AI] ‚úÖ ML –º–æ–¥–µ–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã (Scikit-learn)")
            except Exception as e:
                print(f"[IRIS AI] ‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ML: {e}")
    
    def _init_ai_modules(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ò–ò-–º–æ–¥—É–ª–µ–π"""
        print("[IRIS AI] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ò–ò-–º–æ–¥—É–ª–µ–π...")
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç–µ–ª—å wake word
        self.adaptive_wake_detector = AdaptiveWakeDetector()
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        self.context_processor = ContextProcessor()
        
        # –ú–æ–¥—É–ª—å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è
        if self.enable_self_learning:
            self.learning_module = SelfLearningModule(self.base_dir)
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        if self.enable_emotion_recognition and AUDIO_ML_AVAILABLE:
            self.emotion_analyzer = EmotionAnalyzer()
        
        print("[IRIS AI] ‚úÖ –ò–ò-–º–æ–¥—É–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    
    def _init_vosk(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Vosk —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª—å—é"""
        model_paths = [
            "models/vosk-model-ru-0.22",
            self.base_dir / "models/vosk-model-ru-0.22",
            Path.home() / ".vosk/vosk-model-ru-0.22",
            "/usr/share/vosk/vosk-model-ru-0.22"
        ]
        
        for path in model_paths:
            if os.path.exists(path):
                try:
                    self.vosk_model = Model(str(path))
                    self.vosk_recognizer = KaldiRecognizer(self.vosk_model, 16000)
                    self.vosk_recognizer.SetWords(True)
                    self.speech_engines['vosk'] = self.vosk_recognizer
                    print(f"[IRIS AI] ‚úÖ Vosk –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {path}")
                    return
                except Exception as e:
                    print(f"[IRIS AI] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ Vosk –º–æ–¥–µ–ª–∏: {e}")
        
        print("[IRIS AI] ‚ö†Ô∏è –ú–æ–¥–µ–ª—å Vosk –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
    
    def _init_speech_recognition(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SpeechRecognition —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
        try:
            self.sr_recognizer = sr.Recognizer()
            
            # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            self.sr_recognizer.dynamic_energy_threshold = True
            self.sr_recognizer.energy_threshold = 3000
            self.sr_recognizer.pause_threshold = 0.8
            self.sr_recognizer.phrase_threshold = 0.3
            self.sr_recognizer.non_speaking_duration = 0.5
            
            self.speech_engines['google'] = self.sr_recognizer
            print("[IRIS AI] ‚úÖ SpeechRecognition –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"[IRIS AI] –û—à–∏–±–∫–∞ SpeechRecognition: {e}")
    
    def _init_yandex_speechkit(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –Ø–Ω–¥–µ–∫—Å SpeechKit"""
        # API –∫–ª—é—á –º–æ–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
        self.yandex_api_key = os.getenv('YANDEX_SPEECHKIT_API_KEY', '')
        
        if self.yandex_api_key:
            self.speech_engines['yandex'] = True
            print("[IRIS AI] ‚úÖ –Ø–Ω–¥–µ–∫—Å SpeechKit –¥–æ—Å—Ç—É–ø–µ–Ω")
        else:
            print("[IRIS AI] ‚ö†Ô∏è –Ø–Ω–¥–µ–∫—Å SpeechKit API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    def _init_audio_system(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –∞—É–¥–∏–æ—Å–∏—Å—Ç–µ–º—ã"""
        print("[IRIS AI] –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ—Å–∏—Å—Ç–µ–º—ã...")
        
        self.audio_processors = {}
        
        if PYAUDIO_AVAILABLE:
            try:
                self.pyaudio_instance = pyaudio.PyAudio()
                
                # –ü–æ–∏—Å–∫ –ª—É—á—à–µ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
                self.audio_device = self._select_best_audio_device()
                
                # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ—Ç–æ–∫–∞
                self.stream_config = {
                    'format': pyaudio.paInt16,
                    'channels': 1,
                    'rate': 16000,
                    'frames_per_buffer': 2048,
                    'input': True,
                    'output': False,
                    'input_device_index': self.audio_device['index']
                }
                
                print(f"[IRIS AI] ‚úÖ –ê—É–¥–∏–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.audio_device.get('name', 'unknown')}")
                
            except Exception as e:
                print(f"[IRIS AI] –û—à–∏–±–∫–∞ PyAudio: {e}")
        
        if AUDIO_ML_AVAILABLE:
            try:
                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å librosa
                self.audio_processors['enhancer'] = AudioEnhancer()
                print("[IRIS AI] ‚úÖ –ê—É–¥–∏–æ–æ–±—Ä–∞–±–æ—Ç—á–∏–∫ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            except Exception as e:
                print(f"[IRIS AI] –û—à–∏–±–∫–∞ –∞—É–¥–∏–æ–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
    
    def _select_best_audio_device(self) -> Dict:
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –∞—É–¥–∏–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫"""
        if not PYAUDIO_AVAILABLE:
            return {}
        
        devices = []
        
        for i in range(self.pyaudio_instance.get_device_count()):
            info = self.pyaudio_instance.get_device_info_by_index(i)
            
            if info.get('maxInputChannels', 0) > 0:
                # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
                score = 0
                
                # –í—ã—Å–æ–∫–∞—è —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
                score += info.get('defaultSampleRate', 0) / 44100
                
                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–Ω–∞–ª–æ–≤
                score += info.get('maxInputChannels', 0) / 2
                
                # –ù–∏–∑–∫–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                latency = info.get('defaultLowInputLatency', 0.1)
                score += 1.0 - min(latency, 0.5) * 2
                
                devices.append({
                    'index': i,
                    'info': info,
                    'score': score,
                    'name': info.get('name', f'Device {i}')
                })
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ—Ü–µ–Ω–∫–µ
        devices.sort(key=lambda x: x['score'], reverse=True)
        
        return devices[0] if devices else {}
    
    def _load_neural_weights(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π"""
        model_path = self.paths['models'] / "neural_weights.pth"
        
        if model_path.exists():
            try:
                checkpoint = torch.load(model_path, map_location='cpu')
                
                for model_name, model in self.neural_models.items():
                    if model_name in checkpoint:
                        model.load_state_dict(checkpoint[model_name])
                        print(f"[IRIS AI] –í–µ—Å–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è {model_name}")
                
                print("[IRIS AI] ‚úÖ –í–µ—Å–∞ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
            except Exception as e:
                print(f"[IRIS AI] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤: {e}")
    
    def _load_config(self, config_path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
                
            # –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
            if 'user_profile' in config:
                for key, value in config['user_profile'].items():
                    if hasattr(self.user_profile, key):
                        setattr(self.user_profile, key, value)
            
            if 'neural_config' in config:
                for key, value in config['neural_config'].items():
                    if hasattr(self.neural_config, key):
                        setattr(self.neural_config, key, value)
            
            print(f"[IRIS AI] –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {config_path}")
            
        except Exception as e:
            print(f"[IRIS AI] –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
    
    def _print_system_info(self):
        """–í—ã–≤–æ–¥ –ø–æ–¥—Ä–æ–±–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∏—Å—Ç–µ–º–µ"""
        print("\n" + "="*70)
        print("üß† IRIS AI - –°–ò–°–¢–ï–ú–ù–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø")
        print("="*70)
        
        print(f"–†–µ–∂–∏–º –ò–ò: {self.ai_mode}")
        print(f"–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ: {'‚úÖ –í–ö–õ' if self.enable_self_learning else '‚ùå –í–´–ö–õ'}")
        print(f"–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —ç–º–æ—Ü–∏–π: {'‚úÖ –í–ö–õ' if self.enable_emotion_recognition else '‚ùå –í–´–ö–õ'}")
        print(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∞—è –æ—Å–≤–µ–¥–æ–º–ª–µ–Ω–Ω–æ—Å—Ç—å: {'‚úÖ –í–ö–õ' if self.enable_context_awareness else '‚ùå –í–´–ö–õ'}")
        
        print("\nüìä –î–û–°–¢–£–ü–ù–´–ï –ú–û–î–£–õ–ò:")
        print(f"  ‚Ä¢ Vosk: {'‚úÖ' if 'vosk' in self.speech_engines else '‚ùå'}")
        print(f"  ‚Ä¢ Google Speech: {'‚úÖ' if 'google' in self.speech_engines else '‚ùå'}")
        print(f"  ‚Ä¢ –Ø–Ω–¥–µ–∫—Å SpeechKit: {'‚úÖ' if 'yandex' in self.speech_engines else '‚ùå'}")
        print(f"  ‚Ä¢ PyTorch: {'‚úÖ' if ML_LIBS.get('PYTORCH') else '‚ùå'}")
        print(f"  ‚Ä¢ TensorFlow: {'‚úÖ' if ML_LIBS.get('TENSORFLOW') else '‚ùå'}")
        print(f"  ‚Ä¢ NLP: {'‚úÖ' if NLP_AVAILABLE else '‚ùå'}")
        print(f"  ‚Ä¢ –ê—É–¥–∏–æ–æ–±—Ä–∞–±–æ—Ç–∫–∞: {'‚úÖ' if AUDIO_ML_AVAILABLE else '‚ùå'}")
        
        print(f"\nüíæ –•–†–ê–ù–ò–õ–ò–©–ï: {self.base_dir}")
        print("="*70)
    
    # ============================================
    # –û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´ –û–ë–†–ê–ë–û–¢–ö–ò
    # ============================================
    
    def start(self):
        """–ó–∞–ø—É—Å–∫ –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        if self.is_running:
            print("[IRIS AI] –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç —É–∂–µ –∑–∞–ø—É—â–µ–Ω")
            return
        
        print("üöÄ –ó–∞–ø—É—Å–∫ IRIS AI...")
        self.is_running = True
        
        # –ó–∞–ø—É—Å–∫ –ø–æ—Ç–æ–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∫–∏
        threads_to_start = [
            ('audio_capture', self._audio_capture_loop),
            ('speech_processing', self._speech_processing_loop),
            ('ai_processing', self._ai_processing_loop),
            ('command_handler', self._command_handling_loop),
            ('learning', self._learning_loop)
        ]
        
        for name, target in threads_to_start:
            thread = threading.Thread(target=target, daemon=True, name=f"iris_{name}")
            thread.start()
            self.threads[name] = thread
        
        # –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
        asyncio.run(self._async_tasks_loop())
        
        print("‚úÖ IRIS AI –∑–∞–ø—É—â–µ–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç")
    
    def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        if not self.is_running:
            return
        
        print("[IRIS AI] –û—Å—Ç–∞–Ω–æ–≤–∫–∞...")
        self.is_running = False
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        self._save_state()
        
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ—Ç–æ–∫–æ–≤
        for thread in self.threads.values():
            if thread.is_alive():
                thread.join(timeout=2.0)
        
        # –ó–∞–∫—Ä—ã—Ç–∏–µ –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫–∞
        if hasattr(self, 'audio_stream') and self.audio_stream:
            self.audio_stream.stop_stream()
            self.audio_stream.close()
        
        print("‚úÖ IRIS AI –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    def _audio_capture_loop(self):
        """–¶–∏–∫–ª –∑–∞—Ö–≤–∞—Ç–∞ –∞—É–¥–∏–æ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π"""
        print("[IRIS AI] –ó–∞–ø—É—Å–∫ –∑–∞—Ö–≤–∞—Ç–∞ –∞—É–¥–∏–æ...")
        
        try:
            self.audio_stream = self.pyaudio_instance.open(**self.stream_config)
            self.audio_stream.start_stream()
            
            audio_buffer = []
            buffer_duration = 0.5  # 500 –º—Å –±—É—Ñ–µ—Ä
            chunk_size = self.stream_config['frames_per_buffer']
            
            while self.is_running:
                try:
                    # –ß—Ç–µ–Ω–∏–µ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö
                    data = self.audio_stream.read(chunk_size, exception_on_overflow=False)
                    audio_buffer.append(data)
                    
                    # –ö–æ–≥–¥–∞ –Ω–∞–∫–æ–ø–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
                    if len(audio_buffer) >= (16000 * buffer_duration) / chunk_size:
                        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –±—É—Ñ–µ—Ä
                        audio_data = b''.join(audio_buffer)
                        
                        # –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –∑–≤—É–∫–∞
                        if 'enhancer' in self.audio_processors:
                            enhanced_audio = self.audio_processors['enhancer'].process(audio_data)
                            audio_data = enhanced_audio
                        
                        # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ –æ—á–µ—Ä–µ–¥—å –æ–±—Ä–∞–±–æ—Ç–∫–∏
                        self.audio_queue.put({
                            'data': audio_data,
                            'timestamp': time.time(),
                            'sample_rate': 16000
                        })
                        
                        # –û—á–∏—Å—Ç–∫–∞ –±—É—Ñ–µ—Ä–∞
                        audio_buffer = []
                        
                except Exception as e:
                    print(f"[IRIS AI] –û—à–∏–±–∫–∞ –∑–∞—Ö–≤–∞—Ç–∞ –∞—É–¥–∏–æ: {e}")
                    time.sleep(0.1)
                    
        except Exception as e:
            print(f"[IRIS AI] –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∞—É–¥–∏–æ: {e}")
    
    def _speech_processing_loop(self):
        """–¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏ —Å –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–º–∏ –¥–≤–∏–∂–∫–∞–º–∏"""
        print("[IRIS AI] –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏...")
        
        while self.is_running:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∞—É–¥–∏–æ –∏–∑ –æ—á–µ—Ä–µ–¥–∏
                audio_packet = self.audio_queue.get(timeout=0.5)
                audio_data = audio_packet['data']
                timestamp = audio_packet['timestamp']
                
                # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Ä–∞–∑–Ω—ã–º–∏ –¥–≤–∏–∂–∫–∞–º–∏
                recognition_results = []
                
                # Vosk —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
                if 'vosk' in self.speech_engines:
                    vosk_result = self._recognize_with_vosk(audio_data)
                    if vosk_result:
                        vosk_result['engine'] = 'vosk'
                        recognition_results.append(vosk_result)
                
                # Google —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
                if 'google' in self.speech_engines:
                    google_result = self._recognize_with_google(audio_data)
                    if google_result:
                        google_result['engine'] = 'google'
                        recognition_results.append(google_result)
                
                # –Ø–Ω–¥–µ–∫—Å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
                if 'yandex' in self.speech_engines:
                    yandex_result = self._recognize_with_yandex(audio_data)
                    if yandex_result:
                        yandex_result['engine'] = 'yandex'
                        recognition_results.append(yandex_result)
                
                # –í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                if recognition_results:
                    best_result = self._select_best_recognition(recognition_results)
                    
                    # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –ò–ò
                    self._process_with_ai(best_result)
                    
            except queue.Empty:
                continue
            except Exception as e:
                print(f"[IRIS AI] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ä–µ—á–∏: {e}")
    
    def _ai_processing_loop(self):
        """–¶–∏–∫–ª –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å –Ω–µ–π—Ä–æ—Å–µ—Ç—è–º–∏"""
        print("[IRIS AI] –ó–∞–ø—É—Å–∫ –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏...")
        
        while self.is_running:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –æ—á–µ—Ä–µ–¥—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
                time.sleep(0.1)
                
                # –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –≤ —Ñ–æ–Ω–æ–≤–æ–º —Ä–µ–∂–∏–º–µ
                if self.enable_emotion_recognition:
                    self._update_emotion_state()
                
                # –ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
                if self.enable_self_learning:
                    self._adapt_to_user()
                
            except Exception as e:
                print(f"[IRIS AI] –û—à–∏–±–∫–∞ –ò–ò-–æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
    
    def _command_handling_loop(self):
        """–¶–∏–∫–ª –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–∞–Ω–¥ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"""
        print("[IRIS AI] –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–∞–Ω–¥...")
        
        while self.is_running:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –∫–æ–º–∞–Ω–¥—É –∏–∑ –æ—á–µ—Ä–µ–¥–∏
                priority, timestamp, command_data = self.command_queue.get(timeout=0.5)
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                processed_command = self._process_command_with_context(command_data)
                
                # –í—ã–∑–æ–≤ –∫–æ–ª–ª–±—ç–∫–æ–≤
                self._trigger_callbacks('command', processed_command)
                
                # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                self._update_context(processed_command)
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"[IRIS AI] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–æ–º–∞–Ω–¥: {e}")
    
    def _learning_loop(self):
        """–¶–∏–∫–ª —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è"""
        if not self.enable_self_learning:
            return
        
        print("[IRIS AI] –ó–∞–ø—É—Å–∫ —Ü–∏–∫–ª–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è...")
        
        learning_interval = 300  # 5 –º–∏–Ω—É—Ç
        
        while self.is_running:
            try:
                time.sleep(learning_interval)
                
                # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                training_data = self._collect_training_data()
                
                # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
                if training_data:
                    self._train_models(training_data)
                
                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
                self._save_learning_progress()
                
            except Exception as e:
                print(f"[IRIS AI] –û—à–∏–±–∫–∞ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è: {e}")
    
    async def _async_tasks_loop(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π —Ü–∏–∫–ª –¥–ª—è –∑–∞–¥–∞—á, —Ç—Ä–µ–±—É—é—â–∏—Ö asyncio"""
        print("[IRIS AI] –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á...")
        
        tasks = [
            self._async_event_processor(),
            self._async_network_monitor(),
            self._async_performance_monitor()
        ]
        
        await asyncio.gather(*tasks)
    
    # ============================================
    # –ú–ï–¢–û–î–´ –†–ê–°–ü–û–ó–ù–ê–í–ê–ù–ò–Ø –†–ï–ß–ò
    # ============================================
    
    def _recognize_with_vosk(self, audio_data: bytes) -> Optional[Dict]:
        """–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Vosk"""
        try:
            if self.vosk_recognizer.AcceptWaveform(audio_data):
                result = json.loads(self.vosk_recognizer.Result())
                text = result.get('text', '').strip()
                
                if text:
                    confidence = result.get('confidence', 0.0)
                    
                    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
                    words = result.get('result', [])
                    word_timings = [(w.get('word'), w.get('start'), w.get('end')) for w in words]
                    
                    return {
                        'text': text,
                        'confidence': confidence,
                        'language': 'ru',
                        'timings': word_timings,
                        'raw_result': result
                    }
        except Exception as e:
            print(f"[IRIS AI] –û—à–∏–±–∫–∞ Vosk: {e}")
        
        return None
    
    def _recognize_with_google(self, audio_data: bytes) -> Optional[Dict]:
        """–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é Google Speech"""
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç SpeechRecognition
            audio = sr.AudioData(audio_data, 16000, 2)
            
            # –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ
            text = self.sr_recognizer.recognize_google(audio, language="ru-RU")
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
            alternatives = []
            try:
                raw_result = self.sr_recognizer.recognize_google(audio, language="ru-RU", show_all=True)
                if isinstance(raw_result, dict) and 'alternative' in raw_result:
                    alternatives = [alt.get('transcript', '') for alt in raw_result['alternative'][1:]]
            except:
                pass
            
            return {
                'text': text,
                'confidence': 0.85,  # Google –Ω–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç confidence –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
                'language': 'ru',
                'alternatives': alternatives
            }
            
        except sr.UnknownValueError:
            return None
        except sr.RequestError as e:
            print(f"[IRIS AI] –û—à–∏–±–∫–∞ Google API: {e}")
            return None
        except Exception as e:
            print(f"[IRIS AI] –û—à–∏–±–∫–∞ Google —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è: {e}")
            return None
    
    def _recognize_with_yandex(self, audio_data: bytes) -> Optional[Dict]:
        """–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é –Ø–Ω–¥–µ–∫—Å SpeechKit"""
        if not self.yandex_api_key:
            return None
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
            url = "https://stt.api.cloud.yandex.net/speech/v1/stt:recognize"
            
            headers = {
                "Authorization": f"Api-Key {self.yandex_api_key}",
            }
            
            params = {
                "lang": "ru-RU",
                "sampleRateHertz": "16000",
                "format": "lpcm",
                "profanityFilter": "false"
            }
            
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞
            response = requests.post(url, headers=headers, params=params, data=audio_data)
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('result', '')
                
                if text:
                    return {
                        'text': text,
                        'confidence': 0.9,  # –Ø–Ω–¥–µ–∫—Å –æ–±—ã—á–Ω–æ –¥–∞–µ—Ç –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å
                        'language': 'ru',
                        'service': 'yandex'
                    }
        
        except Exception as e:
            print(f"[IRIS AI] –û—à–∏–±–∫–∞ –Ø–Ω–¥–µ–∫—Å SpeechKit: {e}")
        
        return None
    
    def _select_best_recognition(self, results: List[Dict]) -> Dict:
        """–í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è"""
        if not results:
            return {}
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        scored_results = []
        
        for result in results:
            score = result.get('confidence', 0.0)
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ–∞–∫—Ç–æ—Ä—ã
            engine_weights = {
                'yandex': 1.1,  # –Ø–Ω–¥–µ–∫—Å –ª—É—á—à–µ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
                'vosk': 1.0,
                'google': 0.9
            }
            
            engine = result.get('engine', '')
            if engine in engine_weights:
                score *= engine_weights[engine]
            
            # –ù–∞–∫–∞–∑–∞–Ω–∏–µ –∑–∞ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
            text_length = len(result.get('text', ''))
            if text_length < 3:
                score *= 0.5
            
            scored_results.append((score, result))
        
        # –í—ã–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º score
        best_score, best_result = max(scored_results, key=lambda x: x[0])
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞
        print(f"[IRIS AI] –í—ã–±—Ä–∞–Ω —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç {best_result.get('engine')} "
              f"(–æ—Ü–µ–Ω–∫–∞: {best_score:.2f}): {best_result.get('text', '')[:50]}...")
        
        return best_result
    
    # ============================================
    # –ò–ò-–û–ë–†–ê–ë–û–¢–ö–ê –ò –°–ê–ú–û–û–ë–£–ß–ï–ù–ò–ï
    # ============================================
    
    def _process_with_ai(self, recognition_result: Dict):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —Å –ø–æ–º–æ—â—å—é –ò–ò"""
        if not recognition_result:
            return
        
        text = recognition_result.get('text', '')
        confidence = recognition_result.get('confidence', 0.0)
        
        # –ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –≤ —Ç–µ–∫—Å—Ç–µ
        if self.enable_emotion_recognition:
            emotion_analysis = self._analyze_emotion_from_text(text)
            self._update_emotion_state(emotion_analysis)
        
        # –ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏–π
        intent_analysis = self._analyze_intent(text)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ wake word —Å –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–º –¥–µ—Ç–µ–∫—Ç–æ—Ä–æ–º
        wake_detected, cleaned_text = self._adaptive_wake_detection(text, confidence)
        
        if wake_detected:
            print(f"üîî [IRIS AI] Wake word –æ–±–Ω–∞—Ä—É–∂–µ–Ω! (–∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º)")
            self._trigger_callbacks('wake', {})
            
            # –ê–∫—Ç–∏–≤–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è
            self._activate_listening_mode()
            
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã –ø–æ—Å–ª–µ wake word
            if cleaned_text:
                self._process_command(cleaned_text, intent_analysis)
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∞–∫—Ç–∏–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ
        elif self._is_listening_active():
            self._process_command(text, intent_analysis)
        
        # –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        if self.enable_self_learning:
            self._collect_learning_sample(text, recognition_result)
    
    def _analyze_emotion_from_text(self, text: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –ø–æ —Ç–µ–∫—Å—Ç—É"""
        emotion_scores = {
            'neutral': 0.5,
            'happy': 0.0,
            'sad': 0.0,
            'angry': 0.0,
            'excited': 0.0
        }
        
        # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞
        emotion_words = {
            'happy': ['—Ö–æ—Ä–æ—à–æ', '–æ—Ç–ª–∏—á–Ω–æ', '–ø—Ä–µ–∫—Ä–∞—Å–Ω–æ', '—Ä–∞–¥', '—Å—á–∞—Å—Ç–ª–∏–≤', '—É—Ä–∞', '—Å—É–ø–µ—Ä'],
            'sad': ['–ø–ª–æ—Ö–æ', '–≥—Ä—É—Å—Ç–Ω–æ', '–ø–µ—á–∞–ª—å–Ω–æ', '—Ç–æ—Å–∫–ª–∏–≤–æ', '–∂–∞–ª—å'],
            'angry': ['–∑–ª–æ–π', '—Å–µ—Ä–¥–∏—Ç', '—Ä–∞–∑–æ–∑–ª–∏–ª—Å—è', '–±–µ—Å–∏—Ç', '—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω'],
            'excited': ['–≤–æ–ª–Ω—É—é—Å—å', '–≤–∑–≤–æ–ª–Ω–æ–≤–∞–Ω', '–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ', '–æ–∂–∏–¥–∞—é', '–Ω–µ —Ç–µ—Ä–ø–∏—Ç—Å—è']
        }
        
        text_lower = text.lower()
        
        for emotion, words in emotion_words.items():
            for word in words:
                if word in text_lower:
                    emotion_scores[emotion] += 0.2
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–π —ç–º–æ—Ü–∏–∏
        dominant_emotion = max(emotion_scores.items(), key=lambda x: x[1])
        
        return {
            'emotion': dominant_emotion[0],
            'confidence': dominant_emotion[1],
            'scores': emotion_scores
        }
    
    def _analyze_intent(self, text: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞–º–µ—Ä–µ–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–µ"""
        if hasattr(self, 'intent_recognizer'):
            return self.intent_recognizer.extract_intent(text)
        
        # –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        text_lower = text.lower()
        intent = "unknown"
        
        for cmd_type, cmd_info in self.SMART_COMMANDS.items():
            for action in cmd_info['actions']:
                if action in text_lower:
                    intent = cmd_type
                    break
        
        return {
            'intent': intent,
            'confidence': 0.7 if intent != "unknown" else 0.0,
            'entities': []
        }
    
    def _adaptive_wake_detection(self, text: str, confidence: float) -> Tuple[bool, str]:
        """–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ wake word —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º"""
        if not text:
            return False, ""
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä
        if hasattr(self, 'adaptive_wake_detector'):
            return self.adaptive_wake_detector.detect(text, confidence, self.user_profile)
        
        # –ë–∞–∑–æ–≤–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
        text_lower = text.lower()
        
        for variant in self.WAKE_WORD_VARIANTS:
            if variant in text_lower:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ wake word
                start_idx = text_lower.find(variant)
                cleaned_text = text_lower[start_idx + len(variant):].strip()
                return True, cleaned_text
        
        return False, text_lower
    
    def _process_command(self, command: str, intent_analysis: Dict):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–æ–º–∞–Ω–¥—ã —Å –ò–ò-–∞–Ω–∞–ª–∏–∑–æ–º"""
        if not command:
            return
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        contextual_command = self._apply_context_to_command(command, intent_analysis)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –æ—á–µ—Ä–µ–¥—å –∫–æ–º–∞–Ω–¥
        priority = self._calculate_command_priority(command, intent_analysis)
        self.command_queue.put((priority, time.time(), {
            'command': command,
            'contextual': contextual_command,
            'intent': intent_analysis,
            'emotion': self.emotion_state.emotion,
            'timestamp': time.time()
        }))
        
        print(f"üí≠ [IRIS AI] –û–±—Ä–∞–±–æ—Ç–∞–Ω–∞ –∫–æ–º–∞–Ω–¥–∞: {command}")
        print(f"   üéØ –ù–∞–º–µ—Ä–µ–Ω–∏–µ: {intent_analysis.get('intent', 'unknown')}")
        print(f"   üòä –≠–º–æ—Ü–∏—è: {self.emotion_state.emotion}")
    
    def _apply_context_to_command(self, command: str, intent: Dict) -> str:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∫ –∫–æ–º–∞–Ω–¥–µ"""
        if not self.enable_context_awareness:
            return command
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
        context_info = self.context_processor.get_context_for_command(command, intent)
        
        enhanced_command = {
            'raw': command,
            'context': context_info,
            'user_profile': self.user_profile.user_id,
            'current_emotion': self.emotion_state.emotion,
            'domain': intent.get('intent', 'general')
        }
        
        return json.dumps(enhanced_command, ensure_ascii=False)
    
    def _calculate_command_priority(self, command: str, intent: Dict) -> int:
        """–†–∞—Å—á–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ –∫–æ–º–∞–Ω–¥—ã"""
        priority = 1  # –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∫–æ–º–∞–Ω–¥
        critical_words = ['—Å—Ç–æ–ø', '–æ—Å—Ç–∞–Ω–æ–≤–∏—Å—å', '–ø–æ–º–æ—â—å', '—Å–ø–∞—Å–∏', '—Ç—Ä–µ–≤–æ–≥–∞']
        if any(word in command.lower() for word in critical_words):
            priority = 0
        
        # –ù–∏–∑–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        info_words = ['—á—Ç–æ', '–∫–∞–∫', '–ø–æ—á–µ–º—É', '—Ä–∞—Å—Å–∫–∞–∂–∏', '–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è']
        if any(word in command.lower() for word in info_words):
            priority = 2
        
        # –£—á–µ—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è
        if self.emotion_state.emotion in ['angry', 'stressed']:
            priority = 0  # –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –ø—Ä–∏ –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã—Ö —ç–º–æ—Ü–∏—è—Ö
        
        return priority
    
    def _update_emotion_state(self, new_analysis: Optional[Dict] = None):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        if not self.enable_emotion_recognition:
            return
        
        if new_analysis:
            self.emotion_state.emotion = new_analysis.get('emotion', 'neutral')
            self.emotion_state.confidence = new_analysis.get('confidence', 0.0)
            self.emotion_state.timestamp = time.time()
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é
            self.emotion_state.history.append({
                'emotion': self.emotion_state.emotion,
                'confidence': self.emotion_state.confidence,
                'timestamp': self.emotion_state.timestamp
            })
            
            # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏—Å—Ç–æ—Ä–∏–∏
            if len(self.emotion_state.history) > 100:
                self.emotion_state.history.pop(0)
            
            # –í—ã–∑–æ–≤ –∫–æ–ª–ª–±—ç–∫–æ–≤
            self._trigger_callbacks('emotion_change', asdict(self.emotion_state))
    
    def _adapt_to_user(self):
        """–ê–¥–∞–ø—Ç–∞—Ü–∏—è –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π"""
        if not self.enable_self_learning:
            return
        
        # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—á–µ—Ç—á–∏–∫–∞ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
        self.user_profile.interaction_count += 1
        
        # –†–∞—Å—á–µ—Ç —É—Ä–æ–≤–Ω—è –∞–¥–∞–ø—Ç–∞—Ü–∏–∏
        base_adaptation = min(self.user_profile.interaction_count / 100, 1.0)
        
        # –£—á–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–π
        success_rate = self._calculate_success_rate()
        adaptation = base_adaptation * success_rate
        
        self.user_profile.adaptation_level = adaptation
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è
        self._save_user_profile()
    
    def _collect_training_data(self) -> Dict:
        """–°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        samples = []
        
        # –°–±–æ—Ä –∞—É–¥–∏–æ –æ–±—Ä–∞–∑—Ü–æ–≤
        if hasattr(self, 'learning_data') and self.learning_data.audio_samples:
            samples.extend(self.learning_data.audio_samples[:10])  # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10
        
        # –°–±–æ—Ä —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
        recent_history = self.get_recent_history(20)
        
        training_data = {
            'audio_samples': samples,
            'transcriptions': [h.get('text', '') for h in recent_history],
            'timestamps': [h.get('timestamp', 0) for h in recent_history],
            'success_rate': self._calculate_success_rate()
        }
        
        return training_data
    
    def _train_models(self, training_data: Dict):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if not training_data or not self.enable_self_learning:
            return
        
        print("[IRIS AI] –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π...")
        
        try:
            # –û–±—É—á–µ–Ω–∏–µ –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ wake word –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
            if hasattr(self, 'adaptive_wake_detector'):
                self.adaptive_wake_detector.train(training_data)
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
            if ML_LIBS.get('PYTORCH', False) and 'transcriptions' in training_data:
                self._train_neural_models(training_data['transcriptions'])
            
            print("[IRIS AI] ‚úÖ –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã")
            
            # –í—ã–∑–æ–≤ –∫–æ–ª–ª–±—ç–∫–æ–≤
            self._trigger_callbacks('learning_update', {
                'timestamp': time.time(),
                'samples_processed': len(training_data.get('transcriptions', [])),
                'success_rate': training_data.get('success_rate', 0.0)
            })
            
        except Exception as e:
            print(f"[IRIS AI] –û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
    
    def _train_neural_models(self, texts: List[str]):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        if not texts or len(texts) < 5:
            return
        
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        # –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ –ø—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ
        try:
            if 'voice_encoder' in self.neural_models:
                model_path = self.paths['models'] / "voice_encoder_latest.pth"
                torch.save(self.neural_models['voice_encoder'].state_dict(), model_path)
            
            print(f"[IRIS AI] –ù–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–µ –º–æ–¥–µ–ª–∏ –æ–±–Ω–æ–≤–ª–µ–Ω—ã –Ω–∞ {len(texts)} –ø—Ä–∏–º–µ—Ä–∞—Ö")
        except Exception as e:
            print(f"[IRIS AI] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π: {e}")
    
    def _collect_learning_sample(self, text: str, recognition_result: Dict):
        """–°–±–æ—Ä –æ–±—Ä–∞–∑—Ü–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        sample = {
            'text': text,
            'recognition_result': recognition_result,
            'timestamp': time.time(),
            'emotion': self.emotion_state.emotion,
            'user_id': self.user_profile.user_id
        }
        
        self.learning_data.transcriptions.append(sample)
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞
        if len(self.learning_data.transcriptions) > 1000:
            self.learning_data.transcriptions.pop(0)
    
    def _calculate_success_rate(self) -> float:
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–∞ —É—Å–ø–µ—à–Ω—ã—Ö —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–π"""
        if not self.learning_data.transcriptions:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º–∏ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å –¥–æ–≤–µ—Ä–∏–µ–º > 0.7
        successful = sum(1 for t in self.learning_data.transcriptions 
                        if t.get('recognition_result', {}).get('confidence', 0) > 0.7)
        
        total = len(self.learning_data.transcriptions)
        
        return successful / total if total > 0 else 0.0
    
    # ============================================
    # –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ –ò –ò–ù–¢–ï–†–§–ï–ô–°
    # ============================================
    
    def add_callback(self, callback_type: str, callback: Callable):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–ª–±—ç–∫–∞"""
        if callback_type in self.callbacks:
            self.callbacks[callback_type].append(callback)
            print(f"[IRIS AI] –î–æ–±–∞–≤–ª–µ–Ω –∫–æ–ª–ª–±—ç–∫ —Ç–∏–ø–∞: {callback_type}")
    
    def _trigger_callbacks(self, callback_type: str, data: Any):
        """–í—ã–∑–æ–≤ –∫–æ–ª–ª–±—ç–∫–æ–≤"""
        if callback_type in self.callbacks:
            for callback in self.callbacks[callback_type]:
                try:
                    callback(data)
                except Exception as e:
                    print(f"[IRIS AI] –û—à–∏–±–∫–∞ –≤ –∫–æ–ª–ª–±—ç–∫–µ {callback_type}: {e}")
    
    def _is_listening_active(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ä–µ–∂–∏–º–∞ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è"""
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—É–¥–µ—Ç –ª–æ–≥–∏–∫–∞ —Ç–∞–π–º–∞—É—Ç–∞
        return True
    
    def _activate_listening_mode(self, duration: float = 10.0):
        """–ê–∫—Ç–∏–≤–∞—Ü–∏—è —Ä–µ–∂–∏–º–∞ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è"""
        self.listening_active_until = time.time() + duration
        print(f"[IRIS AI] –†–µ–∂–∏–º –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—è –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω –Ω–∞ {duration} —Å–µ–∫—É–Ω–¥")
    
    def _update_context(self, command_data: Dict):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ—Å–ª–µ –∫–æ–º–∞–Ω–¥—ã"""
        if not self.enable_context_awareness:
            return
        
        command = command_data.get('command', '') if isinstance(command_data, dict) else command_data
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.ai_context.previous_commands.append(command)
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.ai_context.previous_commands) > 20:
            self.ai_context.previous_commands.pop(0)
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ —Å–∫–æ—Ä–∞
        self.ai_context.context_score = self._calculate_context_score()
    
    def _calculate_context_score(self) -> float:
        """–†–∞—Å—á–µ—Ç —Å–∫–æ—Ä–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏"""
        if not self.ai_context.previous_commands:
            return 0.0
        
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –±–æ–ª—å—à–µ –∫–æ–º–∞–Ω–¥ = –≤—ã—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç
        return min(len(self.ai_context.previous_commands) / 20, 1.0)
    
    def get_system_status(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã"""
        return {
            'status': 'running' if self.is_running else 'stopped',
            'ai_mode': self.ai_mode,
            'user_profile': asdict(self.user_profile),
            'emotion_state': asdict(self.emotion_state),
            'performance': asdict(self.performance_metrics),
            'queue_sizes': {
                'audio': self.audio_queue.qsize(),
                'commands': self.command_queue.qsize()
            },
            'threads_alive': {name: thread.is_alive() for name, thread in self.threads.items()},
            'learning_enabled': self.enable_self_learning,
            'adaptation_level': self.user_profile.adaptation_level,
            'success_rate': self._calculate_success_rate()
        }
    
    def get_recent_history(self, count: int = 10) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö –∑–∞–ø–∏—Å–µ–π –∏—Å—Ç–æ—Ä–∏–∏"""
        return self.learning_data.transcriptions[-count:] if self.learning_data.transcriptions else []
    
    def save_state(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        self._save_state()
    
    def _save_state(self):
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        print("[IRIS AI] –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è...")
        
        try:
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—Ä–æ—Ñ–∏–ª—è
            profile_path = self.paths['profiles'] / f"{self.user_profile.user_id}.json"
            with open(profile_path, 'w', encoding='utf-8') as f:
                json.dump(asdict(self.user_profile), f, indent=2, ensure_ascii=False)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è
            learning_path = self.paths['learning'] / "learning_data.pkl"
            with open(learning_path, 'wb') as f:
                pickle.dump(self.learning_data, f)
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            config_path = self.paths['config'] / "ai_config.json"
            config = {
                'ai_mode': self.ai_mode,
                'neural_config': asdict(self.neural_config),
                'user_profile_id': self.user_profile.user_id,
                'last_save': time.time()
            }
            
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            print("[IRIS AI] ‚úÖ –°–æ—Å—Ç–æ—è–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ")
            
        except Exception as e:
            print(f"[IRIS AI] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è: {e}")
    
    def _save_user_profile(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ñ–∏–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        try:
            profile_path = self.paths['profiles'] / f"{self.user_profile.user_id}.json"
            with open(profile_path, 'w', encoding='utf-8') as f:
                json.dump(asdict(self.user_profile), f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"[IRIS AI] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ—Ñ–∏–ª—è: {e}")
    
    def _save_learning_progress(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        if not self.enable_self_learning:
            return
        
        try:
            progress_path = self.paths['learning'] / "progress.json"
            progress = {
                'total_samples': len(self.learning_data.transcriptions),
                'last_training': time.time(),
                'success_rate': self._calculate_success_rate(),
                'adaptation_level': self.user_profile.adaptation_level
            }
            
            with open(progress_path, 'w', encoding='utf-8') as f:
                json.dump(progress, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"[IRIS AI] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
    
    async def _async_event_processor(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–æ–±—ã—Ç–∏–π"""
        while self.is_running:
            try:
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π –∏–∑ –æ—á–µ—Ä–µ–¥–∏
                await asyncio.sleep(0.1)
            except Exception as e:
                print(f"[IRIS AI] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–æ–±—ã—Ç–∏–π: {e}")
    
    async def _async_network_monitor(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–µ—Ç–µ–≤—ã—Ö —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–π"""
        while self.is_running:
            try:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤
                await asyncio.sleep(60)  # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
                
            except Exception as e:
                print(f"[IRIS AI] –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–µ—Ç–∏: {e}")
    
    async def _async_performance_monitor(self):
        """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        while self.is_running:
            try:
                # –°–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                self.performance_metrics.latency['audio'] = 0.05  # –ü—Ä–∏–º–µ—Ä
                self.performance_metrics.latency['processing'] = 0.1
                
                await asyncio.sleep(5)  # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ 5 —Å–µ–∫—É–Ω–¥
                
            except Exception as e:
                print(f"[IRIS AI] –û—à–∏–±–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}")

# ============================================
# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ö–õ–ê–°–°–´ –ò–ò-–ú–û–î–£–õ–ï–ô
# ============================================

class AdaptiveWakeDetector:
    """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä wake word —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º"""
    
    def __init__(self):
        self.wake_patterns = []
        self.user_specific_patterns = {}
        self.learning_rate = 0.1
        self.threshold = 0.7
        
    def detect(self, text: str, confidence: float, user_profile: UserProfile) -> Tuple[bool, str]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ wake word —Å –∞–¥–∞–ø—Ç–∞—Ü–∏–µ–π"""
        text_lower = text.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        for pattern in self.wake_patterns:
            if pattern in text_lower:
                return True, text_lower.replace(pattern, "", 1).strip()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        user_id = user_profile.user_id
        if user_id in self.user_specific_patterns:
            for pattern in self.user_specific_patterns[user_id]:
                if pattern in text_lower:
                    return True, text_lower.replace(pattern, "", 1).strip()
        
        # Fuzzy matching –¥–ª—è –Ω–æ–≤—ã—Ö –≤–∞—Ä–∏–∞—Ü–∏–π
        wake_variants = ['–∏—Ä–∏—Å', 'iris', '–∏—Ä–∏']
        for variant in wake_variants:
            if variant in text_lower:
                # –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤–æ–º –ø–∞—Ç—Ç–µ—Ä–Ω–µ
                self._learn_pattern(text_lower, variant, user_profile)
                return True, text_lower.replace(variant, "", 1).strip()
        
        return False, text_lower
    
    def _learn_pattern(self, text: str, detected_variant: str, user_profile: UserProfile):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤–æ–º –ø–∞—Ç—Ç–µ—Ä–Ω–µ wake word"""
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤–æ–∫—Ä—É–≥ wake word
        idx = text.find(detected_variant)
        context = text[max(0, idx-5):min(len(text), idx+len(detected_variant)+5)]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        pattern = {
            'text': context,
            'variant': detected_variant,
            'user_id': user_profile.user_id,
            'timestamp': time.time()
        }
        
        self.wake_patterns.append(context)
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
        if len(self.wake_patterns) > 50:
            self.wake_patterns.pop(0)
    
    def train(self, training_data: Dict):
        """–û–±—É—á–µ–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞ –Ω–∞ –¥–∞–Ω–Ω—ã—Ö"""
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        print("[AdaptiveWakeDetector] –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")

class ContextProcessor:
    """–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥"""
    
    def __init__(self):
        self.context_memory = {}
        self.conversation_history = []
        self.entity_tracker = {}
        
    def get_context_for_command(self, command: str, intent: Dict) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è –∫–æ–º–∞–Ω–¥—ã"""
        context = {
            'previous_commands': self.conversation_history[-3:],  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 3 –∫–æ–º–∞–Ω–¥—ã
            'time_of_day': self._get_time_context(),
            'detected_intent': intent.get('intent', 'unknown'),
            'entities': intent.get('entities', []),
            'context_score': self._calculate_relevance(command)
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.conversation_history.append({
            'command': command,
            'intent': intent,
            'timestamp': time.time()
        })
        
        # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.conversation_history) > 100:
            self.conversation_history.pop(0)
        
        return context
    
    def _get_time_context(self) -> Dict:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        now = datetime.now()
        
        return {
            'hour': now.hour,
            'minute': now.minute,
            'day_of_week': now.weekday(),
            'is_working_hours': 9 <= now.hour < 18,
            'is_night': now.hour < 6 or now.hour >= 22
        }
    
    def _calculate_relevance(self, command: str) -> float:
        """–†–∞—Å—á–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫–æ–º–∞–Ω–¥—ã —Ç–µ–∫—É—â–µ–º—É –∫–æ–Ω—Ç–µ–∫—Å—Ç—É"""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
        if not self.conversation_history:
            return 0.0
        
        last_command = self.conversation_history[-1].get('command', '').lower()
        current_command = command.lower()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–π —Å–≤—è–∑–∏
        common_words = set(last_command.split()) & set(current_command.split())
        
        return len(common_words) / max(len(set(last_command.split())), 1)

class SelfLearningModule:
    """–ú–æ–¥—É–ª—å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self, base_dir: Path):
        self.base_dir = base_dir
        self.learning_data = []
        self.model_versions = {}
        
    def add_sample(self, text: str, correct_transcription: str, confidence: float):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞–∑—Ü–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        sample = {
            'input': text,
            'target': correct_transcription,
            'confidence': confidence,
            'timestamp': time.time()
        }
        
        self.learning_data.append(sample)
        
        # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–∏ –Ω–∞–∫–æ–ø–ª–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö
        if len(self.learning_data) >= 100:
            self.train()
    
    def train(self):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if len(self.learning_data) < 10:
            return
        
        print("[SelfLearningModule] –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è...")
        
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ–∞–ª—å–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
        # –ù–∞–ø—Ä–∏–º–µ—Ä, –¥–æ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è
        
        # –ü–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è –æ—á–∏—â–∞–µ–º —á–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
        self.learning_data = self.learning_data[-500:]  # –û—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 500
        
        print(f"[SelfLearningModule] –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –û–±—Ä–∞–∑—Ü–æ–≤: {len(self.learning_data)}")
    
    def save_progress(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
        progress_path = self.base_dir / "learning_progress.pkl"
        
        try:
            with open(progress_path, 'wb') as f:
                pickle.dump({
                    'learning_data': self.learning_data,
                    'model_versions': self.model_versions,
                    'last_trained': time.time()
                }, f)
        except Exception as e:
            print(f"[SelfLearningModule] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

class EmotionAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —ç–º–æ—Ü–∏–π –ø–æ –≥–æ–ª–æ—Å—É"""
    
    def __init__(self):
        self.emotion_models = {}
        self.feature_extractor = None
        
    def analyze_audio(self, audio_data: np.ndarray, sample_rate: int) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ —ç–º–æ—Ü–∏–π –ø–æ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã–º"""
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å librosa
            features = self._extract_audio_features(audio_data, sample_rate)
            
            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —ç–º–æ—Ü–∏–π
            emotion_probs = self._classify_emotion(features)
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–æ–º–∏–Ω–∏—Ä—É—é—â–µ–π —ç–º–æ—Ü–∏–∏
            dominant_emotion = max(emotion_probs.items(), key=lambda x: x[1])
            
            return {
                'emotion': dominant_emotion[0],
                'confidence': dominant_emotion[1],
                'probabilities': emotion_probs,
                'features': features.tolist() if isinstance(features, np.ndarray) else features
            }
            
        except Exception as e:
            print(f"[EmotionAnalyzer] –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            return {'emotion': 'neutral', 'confidence': 0.0}
    
    def _extract_audio_features(self, audio: np.ndarray, sr: int) -> np.ndarray:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞—É–¥–∏–æ-–ø—Ä–∏–∑–Ω–∞–∫–æ–≤"""
        features = []
        
        try:
            # MFCC (Mel-frequency cepstral coefficients)
            mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)
            features.append(mfcc.mean(axis=1))
            
            # Chroma features
            chroma = librosa.feature.chroma_stft(y=audio, sr=sr)
            features.append(chroma.mean(axis=1))
            
            # Spectral contrast
            contrast = librosa.feature.spectral_contrast(y=audio, sr=sr)
            features.append(contrast.mean(axis=1))
            
            # Zero crossing rate
            zcr = librosa.feature.zero_crossing_rate(y=audio)
            features.append([zcr.mean()])
            
            # RMS energy
            rms = librosa.feature.rms(y=audio)
            features.append([rms.mean()])
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            combined = np.concatenate(features)
            
            return combined
            
        except Exception as e:
            print(f"[EmotionAnalyzer] –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return np.zeros(50)  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–ª–µ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    
    def _classify_emotion(self, features: np.ndarray) -> Dict:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —ç–º–æ—Ü–∏–π –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º"""
        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ä–µ–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–∏–∫—Ç–∏–≤–Ω—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞
        return {
            'neutral': 0.3,
            'happy': 0.2,
            'sad': 0.15,
            'angry': 0.1,
            'excited': 0.1,
            'calm': 0.1,
            'stressed': 0.05
        }

class AudioEnhancer:
    """–£–ª—É—á—à–∏—Ç–µ–ª—å –∫–∞—á–µ—Å—Ç–≤–∞ –∞—É–¥–∏–æ"""
    
    def __init__(self):
        self.noise_profile = None
        self.enhancement_params = {
            'noise_reduction': 0.8,
            'gain': 1.2,
            'compression': 0.5
        }
    
    def process(self, audio_data: bytes) -> bytes:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —É–ª—É—á—à–µ–Ω–∏–µ –∞—É–¥–∏–æ"""
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ bytes –≤ numpy array
            audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0
            
            # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π
            enhanced = self._apply_enhancements(audio_array)
            
            # –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ bytes
            enhanced_bytes = (enhanced * 32768.0).astype(np.int16).tobytes()
            
            return enhanced_bytes
            
        except Exception as e:
            print(f"[AudioEnhancer] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")
            return audio_data  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
    
    def _apply_enhancements(self, audio: np.ndarray) -> np.ndarray:
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏–π –∫ –∞—É–¥–∏–æ"""
        # –®—É–º–æ–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ
        if self.noise_profile is not None:
            audio = self._reduce_noise(audio)
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏
        audio = self._normalize_volume(audio)
        
        # –ö–æ–º–ø—Ä–µ—Å—Å–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞
        audio = self._compress_dynamic_range(audio)
        
        return audio
    
    def _reduce_noise(self, audio: np.ndarray) -> np.ndarray:
        """–ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ —à—É–º–∞"""
        # –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ—Ä–æ–≥–æ–≤–æ–≥–æ –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è —à—É–º–∞
        threshold = np.std(audio) * self.enhancement_params['noise_reduction']
        audio[abs(audio) < threshold] = 0
        return audio
    
    def _normalize_volume(self, audio: np.ndarray) -> np.ndarray:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏"""
        max_amp = np.max(np.abs(audio))
        if max_amp > 0:
            audio = audio / max_amp * self.enhancement_params['gain']
        return np.clip(audio, -1.0, 1.0)
    
    def _compress_dynamic_range(self, audio: np.ndarray) -> np.ndarray:
        """–ö–æ–º–ø—Ä–µ—Å—Å–∏—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –¥–∏–∞–ø–∞–∑–æ–Ω–∞"""
        compression = self.enhancement_params['compression']
        return np.tanh(audio * compression) / np.tanh(compression)

# ============================================
# –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ò –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø
# ============================================

def create_iris_ai(config: Optional[Dict] = None) -> IRISVoiceAI:
    """
    –§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è IRIS AI –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    
    Args:
        config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    
    Returns:
        IRISVoiceAI: –≠–∫–∑–µ–º–ø–ª—è—Ä –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    """
    config = config or {}
    
    ai = IRISVoiceAI(
        config_path=config.get('config_path'),
        ai_mode=config.get('ai_mode', 'adaptive'),
        neural_config=config.get('neural_config'),
        enable_self_learning=config.get('enable_self_learning', True),
        enable_emotion_recognition=config.get('enable_emotion_recognition', True),
        enable_context_awareness=config.get('enable_context_awareness', True)
    )
    
    return ai

async def demo_iris_ai():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π IRIS AI"""
    print("\n" + "="*70)
    print("üöÄ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø IRIS AI - –£–ú–ù–´–ô –ì–û–õ–û–°–û–í–û–ô –ê–°–°–ò–°–¢–ï–ù–¢")
    print("="*70)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    config = {
        'ai_mode': 'adaptive',
        'enable_self_learning': True,
        'enable_emotion_recognition': True,
        'enable_context_awareness': True
    }
    
    iris = create_iris_ai(config)
    
    # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–ª–±—ç–∫–æ–≤
    def on_wake():
        print("\nüéØ Wake word –æ–±–Ω–∞—Ä—É–∂–µ–Ω! IRIS –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
    
    def on_command(command):
        print(f"\nüí¨ –ü–æ–ª—É—á–µ–Ω–∞ –∫–æ–º–∞–Ω–¥–∞: {command}")
    
    def on_emotion_change(emotion_state):
        print(f"\nüòä –ò–∑–º–µ–Ω–∏–ª–∞—Å—å —ç–º–æ—Ü–∏—è: {emotion_state['emotion']} "
              f"(—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {emotion_state['confidence']:.1%})")
    
    def on_learning_update(update):
        print(f"\nüìö –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–∏—è: –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {update['samples_processed']} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    iris.add_callback('wake', on_wake)
    iris.add_callback('command', on_command)
    iris.add_callback('emotion_change', on_emotion_change)
    iris.add_callback('learning_update', on_learning_update)
    
    # –ó–∞–ø—É—Å–∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    print("\n‚ñ∂Ô∏è –ó–∞–ø—É—Å–∫–∞—é IRIS AI...")
    iris.start()
    
    print("\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:")
    print("   ‚Ä¢ –°–∫–∞–∂–∏—Ç–µ '–ò—Ä–∏—Å' –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏")
    print("   ‚Ä¢ –ó–∞—Ç–µ–º –ø—Ä–æ–∏–∑–Ω–µ—Å–∏—Ç–µ –∫–æ–º–∞–Ω–¥—É (–Ω–∞–ø—Ä–∏–º–µ—Ä: '–ò—Ä–∏—Å, –∫–∞–∫–∞—è –ø–æ–≥–æ–¥–∞?')")
    print("   ‚Ä¢ –°–∫–∞–∂–∏—Ç–µ '–ò—Ä–∏—Å, —Ä–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç'")
    print("   ‚Ä¢ –°–∫–∞–∂–∏—Ç–µ '–ò—Ä–∏—Å, –≤–∫–ª—é—á–∏ –º—É–∑—ã–∫—É'")
    print("   ‚Ä¢ –°–∫–∞–∂–∏—Ç–µ '—Å—Ç–æ–ø' –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–µ–º–æ")
    
    print("\n‚è≥ –î–µ–º–æ –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å 60 —Å–µ–∫—É–Ω–¥...")
    
    # –î–µ–º–æ –Ω–∞ 60 —Å–µ–∫—É–Ω–¥
    try:
        start_time = time.time()
        
        while time.time() - start_time < 60:
            await asyncio.sleep(1)
            
            # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏–π –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç—É—Å–∞
            if int(time.time() - start_time) % 10 == 0:
                status = iris.get_system_status()
                print(f"\nüìä –°—Ç–∞—Ç—É—Å: {status['status']}, "
                      f"–∞–¥–∞–ø—Ç–∞—Ü–∏—è: {status['adaptation_level']:.1%}, "
                      f"—ç–º–æ—Ü–∏—è: {status['emotion_state']['emotion']}")
    
    except KeyboardInterrupt:
        print("\n\nüõë –ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    
    finally:
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        print("\n‚èπÔ∏è –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é IRIS AI...")
        iris.stop()
        
        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        print("\n" + "="*70)
        print("üìà –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –î–ï–ú–û")
        print("="*70)
        
        final_stats = iris.get_system_status()
        for key, value in final_stats.items():
            if key not in ['user_profile', 'emotion_state', 'performance']:
                print(f"  {key}: {value}")
        
        print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print("="*70)

if __name__ == "__main__":
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    print("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π IRIS AI...")
    
    deps = {
        "NumPy": NP_AVAILABLE,
        "PyAudio": PYAUDIO_AVAILABLE,
        "Vosk": VOSK_AVAILABLE,
        "SpeechRecognition": SR_AVAILABLE,
        "Requests": YANDEX_AVAILABLE,
        "PyTorch": ML_LIBS.get('PYTORCH', False),
        "Scikit-learn": ML_LIBS.get('SKLEARN', False),
        "Librosa": AUDIO_ML_AVAILABLE,
        "NLTK": NLP_AVAILABLE
    }
    
    print("\nüì¶ –£–°–¢–ê–ù–û–í–õ–ï–ù–ù–´–ï –ó–ê–í–ò–°–ò–ú–û–°–¢–ò:")
    for dep, available in deps.items():
        status = "‚úÖ" if available else "‚ùå"
        print(f"  {status} {dep}")
    
    # –ó–∞–ø—É—Å–∫ –¥–µ–º–æ
    asyncio.run(demo_iris_ai())